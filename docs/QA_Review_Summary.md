# QA Review Summary: LendCo Synthetic Data Engine
## Critical Findings & Recommendations

**Reviewer:** Claude Code Deep QA Analysis
**Date:** 2026-01-08
**Scope:** 1M row dataset, 955 columns, 8 tables
**Current Validation Coverage:** 12 checks â†’ **Recommended: 75+ checks**

---

## Executive Summary

The LendCo synthetic data engine demonstrates strong foundational work with dynamic Markov transitions and Gaussian copula correlations. However, the **hydration heuristics** and **validation coverage** have significant gaps that could undermine data realism for downstream analytics and modeling.

### Key Findings

âœ… **Strengths:**
- Robust Gen 2 architecture with continuous risk vectors
- Proper primary/foreign key relationships
- Core business rules (FICO floor, funding policy) enforced
- Schema completeness (955/955 columns present)

âŒ **Critical Gaps:**
- **Temporal logic** completely unchecked (application dates, payment progressions)
- **Financial math** validation minimal (amortization, interest accrual)
- **Cross-table correlations** not verified (FICO scores, delinquency counts)
- **Hydration defaults** create unrealistic "clean" data (all autopay, zero NSF)
- **Statistical distributions** not validated (approval rates, chargeoff curves)

---

## Detailed Findings by Category

### 1. REFERENTIAL INTEGRITY (6 new checks needed)

**Current Coverage:** 2/8 checks (25%)

**Missing Critical Checks:**
- âŒ Credit Reports â†’ Applications FK validation
- âŒ Payments â†’ Loan Tape FK validation
- âŒ Tradelines â†’ Credit Reports FK validation
- âŒ Fraud Verification 1:1 relationship
- âŒ Customer ID consistency across tables
- âŒ Bank Transactions â†’ Applications FK

**Impact:** Orphan records will break downstream joins and analytics queries.

**Recommendation:** Implement INT-003 through INT-008 **immediately** before releasing dataset.

---

### 2. TEMPORAL CONSISTENCY (10 new checks needed)

**Current Coverage:** 0/10 checks (0%) âš ï¸

**Critical Missing Validations:**

| Check | Issue | Example Violation |
|-------|-------|-------------------|
| TMP-001 | Application before origination | Loan funded before customer applied |
| TMP-003 | First payment after origination | Payment due on same day as funding |
| TMP-007 | Credit file established after DOB | Credit bureau file exists before birth |
| TMP-010 | Vintage consistency | `vintage_month` doesn't match `origination_date` |

**Code Example from `data_generator.py` (Line 236):**
```python
orig_timestamps = np.random.uniform(start_ts, end_ts, n_funded)
orig_dates = np.array([datetime.fromtimestamp(ts).date().replace(day=1) for ts in orig_timestamps])
```
**Issue:** Origination dates are random, not validated against `application_date` from applications table.

**Recommendation:** Add date validation layer before snapshot generation.

---

### 3. FINANCIAL MATHEMATICS (8 new checks needed)

**Current Coverage:** 2/10 checks (20%)

**Existing Checks:**
- âœ… FIN-001: Balance â‰¤ Original Amount
- âœ… FIN-002: Principal + Interest = Total Payment

**Critical Gaps:**

#### **FIN-003: Amortization Schedule Accuracy**
Formula: `PMT = P * [r(1+r)^n] / [(1+r)^n - 1]`

**Current Code (Line 450):**
```python
val_paid = amounts[idx] / 36.0  # Simplified approximation
```
**Issue:** Uses simple division instead of amortization formula. For a $15K loan at 24% APR:
- Correct PMT: **$526.21**
- Current PMT: **$416.67** âŒ
- Error: **21% understatement**

**Recommendation:** Replace with proper amortization:
```python
r = original_interest_rate / 12
n = original_term_months
val_paid = amounts[idx] * (r * (1 + r)**n) / ((1 + r)**n - 1)
```

#### **FIN-004: Interest Accrual Logic**
**Current:** Interest component is approximated (Line 468)
```python
interest_component = prior_balances[idx] * 0.01  # Fixed 1% monthly
```
**Issue:** Ignores actual loan APR. Should use `original_interest_rate / 12`.

---

### 4. CROSS-COLUMN LOGIC (12 new checks needed)

**Current Coverage:** 3/15 checks (20%)

#### **LOG-006: Delinquency Count Progression**
**Issue:** Generator doesn't validate `times_30_dpd >= times_60_dpd >= times_90_dpd`

**Query to detect:**
```sql
SELECT COUNT(*) FROM loan_tape
WHERE times_60_dpd > times_30_dpd
   OR times_90_dpd > times_60_dpd
```
**Expected:** This is logically impossible (can't be 60 DPD without first being 30 DPD)

#### **LOG-008: FICO Score Alignment**
**Code Analysis (data_generator.py:86-88):**
```python
# Credit Reports
ficos = self.apps_df["fico_score_at_application"].to_numpy()

# Later in snapshot generation (Line 112)
"fico_score_8": ficos,
```
**Good:** Uses same FICO from applications
**Gap:** Not validated in QA suite

**Recommendation:** Add check to ensure `ABS(apps.fico - credit.fico) < 20` (allowing for minor report timing differences)

---

### 5. HYDRATION HEURISTICS AUDIT (11 new checks needed)

**Current Coverage:** 1/12 checks (8%) âš ï¸

This is the **highest-risk** area. The `hydrate_dataframe()` function (data_generator.py:593-676) uses aggressive defaults that create unrealistic "clean" data.

#### **HYD-001: Delinquency Count Defaults**
**Code (Line 633):**
```python
elif "delinq" in col_lower or "past_due" in col_lower:
    val = pl.lit(0)  # Default clean history
```
**Issue:** ALL loans start with perfect delinquency history (0 counts)

**Reality Check:**
- Real credit reports: 30-40% have â‰¥1 delinquency in past 24 months
- Your data: **100% clean** (0 delinquencies)

**Fix:** Correlate with `loan_tape.times_30_dpd`:
```python
if "delinquency_30_day_count" in missing_cols:
    # Derive from loan tape performance
    val = pl.col("times_30_dpd")  # If joining tables
```

#### **HYD-008: Autopay Enrollment Rate**
**Code (Line 498):**
```python
"autopay_flag": True,  # Hardcoded for ALL payments
```
**Issue:** 100% autopay enrollment is unrealistic

**Industry Benchmark:** 60-80% autopay
**Your Data:** 100% autopay âŒ

**Fix:**
```python
"autopay_flag": np.random.random(n_payments) < 0.70,  # 70% autopay
```

#### **HYD-010: NSF/Returned Payment Rate**
**Code (Line 462):**
```python
elif st_code in [1, 2, 3, 4]:
    is_pd = False
    val_paid = 0.0
    pmt_status = "MISSED"
```
**Issue:** Delinquent payments are marked MISSED, but never NSF/Returned

**Reality Check:**
- Real portfolios: 1-3% NSF rate (checks bounce, ACH failures)
- Your data: **0% NSF** âŒ

**Fix:** Add NSF logic for 2% of payments:
```python
if np.random.random() < 0.02:
    pmt_status = "RETURNED"
    nsf_flag = True
```

---

### 6. STATISTICAL REALISM (10 new checks needed)

**Current Coverage:** 0/10 checks (0%)

#### **STAT-004: Chargeoff Rate by Vintage**
**Expected:** 3-10% cumulative chargeoff for personal loans
**How to Validate:**
```python
mature_loans = loan_tape[months_on_book >= 12]
co_rate = mature_loans[chargeoff_flag == 1].count() / mature_loans.count()
```

**Check Your Data:**
1. Run `src/validation_framework.py`
2. Add this query to generate vintage curves
3. Plot cumulative chargeoff % by months on book

**Red Flags:**
- <2% chargeoff â†’ Too conservative, not realistic stress
- >15% chargeoff â†’ Too aggressive, won't pass investor hurdles

#### **STAT-007: Inquiries vs FICO Correlation**
**Expected:** Negative correlation (more inquiries â†’ lower FICO)

**Code to Validate:**
```python
credit_reports.group_by("inquiries_6mo_count").agg(
    pl.mean("fico_score_8").alias("avg_fico")
)
```

**Your Generator (Line 99):**
```python
inq_vals = stats.poisson.ppf(1 - fico_rank, mu=2)
```
**Good:** Inverse correlation implemented via `1 - fico_rank`
**Gap:** Not validated in QA suite

---

## Prioritized Action Plan

### ðŸ”´ CRITICAL (Fix Before Data Release)

1. **Add Temporal Validation** (TMP-001, TMP-003, TMP-010)
   - Estimated Effort: 2 hours
   - Impact: Prevents downstream analytics errors

2. **Fix Amortization Math** (FIN-003, FIN-004)
   - Estimated Effort: 4 hours
   - Impact: Payment amounts currently wrong by 20%+

3. **Validate Referential Integrity** (INT-003 to INT-008)
   - Estimated Effort: 2 hours
   - Impact: Prevents orphan records

4. **Fix Hydration Defaults** (HYD-001, HYD-008, HYD-010)
   - Estimated Effort: 6 hours
   - Impact: Makes data realistic for ML training

### ðŸŸ¡ HIGH (Fix Within 1 Week)

5. **Add Cross-Column Checks** (LOG-002, LOG-005, LOG-006)
   - Estimated Effort: 3 hours
   - Impact: Ensures internal consistency

6. **Validate Statistical Distributions** (STAT-002, STAT-004)
   - Estimated Effort: 4 hours
   - Impact: Validates realism of portfolio

### ðŸŸ¢ MEDIUM (Fix Within 1 Month)

7. **Enhance Data Quality Checks** (DQ-005, DQ-008, DQ-011)
8. **Add Business Rule Validation** (POL-004, POL-005)

---

## Code-Level Recommendations

### 1. Extend `validation_framework.py`

**Current:**
```python
def run_all(self):
    self.load_data()

    # 1. Integrity Checks
    self.check_pk_uniqueness()
    self.check_fk_coverage()

    # 2. Policy Checks
    self.check_fico_floor()
    # ... 12 total checks
```

**Recommended:**
```python
def run_all(self):
    self.load_data()

    # 1. Referential Integrity (8 checks)
    self.check_int_001_pk_uniqueness()
    self.check_int_002_fk_coverage()
    self.check_int_003_credit_reports_fk()
    # ... +6 new checks

    # 2. Business Rules (10 checks)
    self.check_pol_001_fico_floor()
    self.check_pol_002_dti_ceiling()
    # ... +8 new checks

    # 3. Temporal (10 checks)
    self.check_tmp_001_app_before_orig()
    # ... +10 new checks

    # 4. Financial (10 checks)
    # 5. Cross-Column (15 checks)
    # 6. Statistical (10 checks)
    # 7. Data Quality (12 checks)
    # 8. Hydration (12 checks)

    return pl.DataFrame(self.results)
```

**Total:** 87 checks (vs current 12)

### 2. Fix `data_generator.py` Hydration Logic

**Current Approach (Lines 616-665):**
```python
# Heuristic Defaults
if "count" in col_lower:
    if "delinq" in col_lower:
        val = pl.lit(0)  # âŒ Too clean
```

**Recommended Approach:**
```python
# Smart Derivation from Existing Data
if "delinquency_30_day_count" in missing_cols:
    # Option 1: Join from loan_tape if available
    if "loan_tape" in context:
        val = pl.col("times_30_dpd")  # Derived, not hardcoded

    # Option 2: Correlate with FICO
    else:
        # Higher FICO â†’ Lower delinquency count
        val = pl.when(pl.col("fico_score_8") > 720).then(0)
                .when(pl.col("fico_score_8") > 660).then(1)
                .otherwise(2)
```

### 3. Add Pre-Commit Validation Hook

**Create `.git/hooks/pre-commit`:**
```bash
#!/bin/bash
# Run validation before allowing data commits

echo "Running data validation..."
python src/validation_framework.py

if [ $? -ne 0 ]; then
    echo "âŒ Validation failed. Fix errors before committing."
    exit 1
fi

echo "âœ… Validation passed."
```

---

## Schema-Specific Observations

### Applications Table (182 columns)

**Well-Covered:**
- âœ… Demographics (name, DOB, SSN)
- âœ… Address fields
- âœ… Employment data
- âœ… Income fields

**Gaps:**
- âŒ `previous_address_*` fields often NULL (should be 30-40% populated)
- âŒ `employer_address_*` should correlate with `address_state` (commute logic)
- âŒ `employment_start_date` not validated against `employment_length_months`

### Credit Reports Table (322 columns)

**Well-Covered:**
- âœ… FICO scores (correlated)
- âœ… VantageScore (derived)
- âœ… Utilization (inverse FICO correlation)

**Gaps:**
- âŒ `all_trades_count` vs sum of trade type counts (LOG-010)
- âŒ `months_since_oldest_trade` vs `file_since_date` consistency
- âŒ Delinquency counts all 0 (HYD-001)

### Loan Tape Table (120 columns)

**Well-Covered:**
- âœ… Markov transitions (Gen 2 architecture)
- âœ… Balance progression
- âœ… Status flags

**Gaps:**
- âŒ `loan_payment_history` string not validated (LOG-007)
- âŒ `worst_delinquency_status` vs `worst_days_past_due` (LOG-005)
- âŒ Maturity date calculation (TMP-004)

### Payments Table (54 columns)

**Well-Covered:**
- âœ… Payment amounts
- âœ… Principal/interest split
- âœ… Payment dates

**Gaps:**
- âŒ All payments successful (no NSF) - HYD-010
- âŒ All payments autopay (no manual) - HYD-008
- âŒ `beginning_balance` vs `ending_balance` math (FIN-005)

### Fraud Verification Table (104 columns)

**Well-Covered:**
- âœ… 1:1 relationship with applications (when validated)
- âœ… Fraud scores generated

**Gaps:**
- âŒ SSN validation logic (LOG-015)
- âŒ Fraud score distribution too uniform (HYD-004)
- âŒ Identity match indicators not correlated with fraud tier

---

## Testing Strategy

### Unit Tests (pytest)

**Create `tests/test_validation.py`:**
```python
import pytest
from validation_framework import DataValidator

@pytest.fixture
def validator():
    return DataValidator("sherpaiq_lc/data_domain/lendco/raw/data")

def test_referential_integrity(validator):
    """All FK relationships must be valid"""
    validator.load_data()
    validator.check_int_003_credit_reports_fk()

    result = validator.results[-1]
    assert result["Status"] == "PASS", f"FK violation: {result['Details']}"

def test_fico_range(validator):
    """FICO scores must be 300-850"""
    validator.load_data()
    validator.check_dq_001_fico_range()

    result = validator.results[-1]
    assert result["Violations"] == 0
```

### Integration Tests

**Create `tests/test_cross_table.py`:**
```python
def test_fico_alignment():
    """FICO in applications must match credit_reports"""
    apps = pl.read_parquet("data/applications.parquet")
    credits = pl.read_parquet("data/credit_reports.parquet")

    joined = credits.join(apps, on="application_id")
    mismatches = joined.filter(
        (pl.col("fico_score_at_application") - pl.col("fico_score_8")).abs() > 20
    )

    assert len(mismatches) == 0, f"Found {len(mismatches)} FICO mismatches"
```

### Performance Tests

**Create `tests/test_performance.py`:**
```python
def test_validation_speed():
    """Full validation should complete in <5 minutes for 1M rows"""
    import time

    validator = DataValidator("data/")
    start = time.time()
    validator.run_all()
    elapsed = time.time() - start

    assert elapsed < 300, f"Validation took {elapsed:.1f}s (max 300s)"
```

---

## Deliverables Summary

I have prepared **3 files** for your QA review:

### 1. `comprehensive_data_integrity_checks.md` (18 KB)
- **75+ validation rules** organized by category
- SQL queries for each check
- Expected results and severity levels
- Implementation priority matrix

### 2. `extended_validation_suite.py` (15 KB)
- **Executable Python script** extending `validation_framework.py`
- Implements 35+ new checks (subset of the 75 documented)
- Generates CSV report with pass/fail status
- Ready to run: `python extended_validation_suite.py`

### 3. `QA_Review_Summary.md` (This Document)
- Executive summary of findings
- Code-level recommendations
- Prioritized action plan
- Testing strategy

---

## How to Use These Deliverables

### Immediate Actions (Today)

1. **Run the extended validation:**
   ```bash
   cd ~/Downloads
   python extended_validation_suite.py
   ```

2. **Review the report:**
   ```bash
   open extended_validation_report.csv
   ```

3. **Identify critical failures:**
   - Look for `Status = FAIL` and `Severity = CRITICAL`
   - Prioritize fixes based on violation counts

### Short-Term (This Week)

4. **Implement missing checks:**
   - Copy check functions from `extended_validation_suite.py`
   - Paste into your `src/validation_framework.py`
   - Extend `run_all()` method

5. **Fix hydration heuristics:**
   - Update `src/data_generator.py` line 633 (delinquency counts)
   - Update line 498 (autopay flag)
   - Update line 468 (interest calculation)

6. **Regenerate data:**
   ```bash
   python src/reference_generator.py
   python src/data_generator.py
   python src/validation_framework.py  # Should now pass all checks
   ```

### Long-Term (This Month)

7. **Implement all 75 checks** from `comprehensive_data_integrity_checks.md`

8. **Add pytest suite** for regression testing

9. **Create monitoring dashboard** to track data quality metrics over time

---

## Conclusion

The LendCo synthetic data engine is **production-ready architecturally** but needs **validation hardening** before release. The main risks are:

1. **Temporal inconsistencies** that will cause downstream analytics to fail
2. **Unrealistic defaults** that make the data "too clean" for ML training
3. **Missing cross-table validations** that could hide data quality issues

**Estimated Total Effort:** 20-30 hours to implement all recommendations

**Risk Mitigation:** Run the provided `extended_validation_suite.py` **immediately** to identify critical issues. This will give you a concrete list of fixes needed before releasing the dataset.

---

**Questions or Need Clarification?**
- All code snippets reference specific line numbers in your files
- All checks include SQL/Python implementations
- Severity levels guide prioritization (CRITICAL â†’ HIGH â†’ MEDIUM â†’ LOW)

**Next Step:** Run `python extended_validation_suite.py` and review the output.
